{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the tutorial, we will learn how to <u>chunk</u> text. Chunking divides a text into segments, such as noun phrases and verb phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, we will import our raw text, and use the code from the \"Part of Speech\" section of the tutorial to tag all the words with their corresponding parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"255s.txt\",\"rU\") #U is for Universal\n",
    "raw = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(raw)\n",
    "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "sentences = [nltk.pos_tag(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunks can be represented in both tree form, and as IOB tags. We are going to use IOB tags to represent our chunks, because we will be using a tagging algorithm to create our chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to get some data to train our chunker on, and we need to format that data in a way that is useful to us. Let's grab some data from the CONLL 2000 collection. This data comes in a tree form, so we need to convert it into IOB format. Afterwards, we convert it into a tuple so that it is compatible with our tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingSentences = nltk.corpus.conll2000.chunked_sents('train.txt') #grab training data from conll2000\n",
    "trainingSentences = [nltk.chunk.tree2conlltags(tree) for tree in trainingSentences] #convert from tree to IOB\n",
    "#convert from triple to tuple\n",
    "trainingSentences = [[(tag,chunk) for (word, tag, chunk) in chunk_tags] for chunk_tags in trainingSentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see our correctly formatted training data. You may also want to print out the data at different stages of the reformatting process, to see what is happening to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingSentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our chunker on our formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_chunker = nltk.tag.BigramTagger(trainingSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass our chunker some data that does not yet have IOB tags.\n",
    "Our data needs to be a list of part of speech singletons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove the words from our data, leaving only the part of speech tags\n",
    "sentencesIOB = [[(tag) for (word, tag) in chunk_tags] for chunk_tags in sentences] #store stripped data in new list\n",
    "sentencesIOB[0] = bigram_chunker.tag(sentencesIOB[0]) #tag the first sentence in our text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see our new IOB tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentencesIOB[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add our IOB tags to our original sentences with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tempSentence=[]\n",
    "for x in range(0,len(sentences[0])):\n",
    "    tempSentence.append((sentences[0][x][0],sentences[0][x][1],sentencesIOB[0][x][1]))\n",
    "sentences[0] = tempSentence\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
