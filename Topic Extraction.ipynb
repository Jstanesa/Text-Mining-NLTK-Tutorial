{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the tutorial will teach you how to extract topics from documents. We will use the \"gensim\" python library to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import a bunch of stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import *\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to create a few simple test documents for this section of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_a = \"Mount Everest attracts many highly experienced mountaineers, as well as capable climbers willing to hire guides.\"\n",
    "doc_b = \"Everest is not the furthest summit from the centre of the Earth.\"\n",
    "doc_c = \"The first recorded efforts to reach Everest's summit were made by British mountaineers.\"\n",
    "doc_d = \"Biology recognizes the cell as the basic unit of life.\"\n",
    "doc_e = \"Biology began to quickly develop and grow with the improvement of the microscope.\"\n",
    "\n",
    "test_doc = \"Climbing microscopes is hard to cells to do.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform our documents into bags of words. \n",
    "In order to get only the most relevant words, we are going to remove stop words from our documents.\n",
    "We will use a stemmer from NLTK to reduce our words to their stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# create English stop words list\n",
    "stop_words = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all our training documents in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_docs = [doc_a,doc_b,doc_c,doc_d,doc_e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's clean up our documents. We're going to put all our bags of words into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in training_docs:\n",
    "    tokens = nltk.word_tokenize(i)\n",
    "    tokens = [w.lower() for w in tokens if w.isalpha()] #lowercase and remove punctuation\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop] #remove stop words\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens] #stem words\n",
    "    texts.append(stemmed_tokens)# add words to corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create a dictionary that has all the words that appear in our documents in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capabl\n",
      "climber\n",
      "everest\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "print dictionary[1]\n",
    "print dictionary[2]\n",
    "print dictionary[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next line takes our words and transforms them into integers which represent those words' index in our dictionary.\n",
    "It also counts the number of occurrances of each word in each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create and train the topic model we will use to extract our topics. We use Latent Dirichlet Allocation for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_topics will determine how many topics our model will generate. Because our documents fall into two categories, I chose 2 topics.  If your document set has a large variety of topics, you should pick a bigger number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passes will determine how many times the model loops through our corpus. A higher number will take more time, and a lower number will be less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print out the topics that our model has found.\n",
    "num_topics in this case will print out a number of topics, up to the number that we generated when we trained our model. Note that these topics are in no particular order.\n",
    "num_words will show a number of the most relevant words to each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'0.079*everest', u'0.082*biolog']\n",
      "[u'0.079*everest + 0.056*mountain + 0.056*summit + 0.034*attract', u'0.082*biolog + 0.049*quickli + 0.049*improv + 0.049*grow']\n"
     ]
    }
   ],
   "source": [
    "print(lda.print_topics(num_topics=2, num_words=1)) #print topics with most relevant word for each topic\n",
    "print(lda.print_topics(num_topics=2, num_words=4)) #print topics with 4 most relevant words for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these topics to not have any names associated with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the topics of a document outside of our training set. \n",
    "First, we have to process our document into a bag of words using our already defined dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tokens = nltk.word_tokenize(test_doc)\n",
    "test_tokens = [w.lower() for w in test_tokens if w.isalpha()]\n",
    "test_tokens = [i for i in tokens if not i in stop_words]    \n",
    "test_tokens = [stemmer.stem(i) for i in test_tokens]\n",
    "test_tokens = dictionary.doc2bow(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let our model extract the topics from our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.064127751544000258), (1, 0.9358722484559997)]\n"
     ]
    }
   ],
   "source": [
    "print(lda[test_tokens]) #feed our test document to our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tuples represent (topic, relevence) pairs. The first number is the number of the topic that the document contains, and the second number is the probability that the document really does contain that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try messing around with the test document and seeing how the results change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
